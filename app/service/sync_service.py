# app/service/sync_service.py
import pandas as pd
import datetime
import os
from app.service.vector_service import VectorService
from app.core.logger import logger

class SyncService:
    """
    CSV íŒŒì¼ì˜ ë‚´ìš©ì„ Vector DBì™€ ë™ê¸°í™”í•˜ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤
    """
    def __init__(self, vector_service: VectorService):
        self.vector_service = vector_service

    def sync_csv_to_db(self, file_path: str):
        """
        CSV íŒŒì¼ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ Vector DBì— ìµœì‹ í™”(ì‚­ì œ í›„ ì¶”ê°€)í•©ë‹ˆë‹¤.
        
        Args:
            file_path (str): ë¶„ì„í•  CSV íŒŒì¼ ê²½ë¡œ.
        """
        base_name = os.path.basename(file_path)
        parts = base_name.split('_')
        
        if len(parts) < 3:
            logger.error(f"íŒŒì¼ëª… í˜•ì‹ ì˜¤ë¥˜: {base_name} (í•„ìˆ˜ í˜•ì‹: ì¹´í…Œê³ ë¦¬_SNS_... .csv)")
            return

        category = parts[0]
        sns_name = parts[1]

        logger.info(f"ğŸ”„ [{category} | {sns_name}] íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ DBì— ë™ê¸°í™”í•©ë‹ˆë‹¤...")
        try:
            self.vector_service.delete_by_metadata(filter={"$and": [{"category": category}, {"sns": sns_name}]})
            logger.info(f"ê¸°ì¡´ '{category}' ì¹´í…Œê³ ë¦¬, '{sns_name}' SNS ë°ì´í„° ì‚­ì œ ì™„ë£Œ.")
        except Exception as e:
            logger.warning(f"â„¹ï¸ ì´ì „ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆê±°ë‚˜ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {e}")

        df = pd.read_csv(file_path)
        documents, metadatas, ids = [], [], []
        save_time = datetime.datetime.now().isoformat()

        df_filtered = df[df['trend_keywords'].notna() & (df['trend_keywords'] != '')]

        for _, row in df_filtered.iterrows():
            keywords_str = row['trend_keywords']
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œë¥¼ ë¶„ë¦¬í•˜ê³ , ê° í‚¤ì›Œë“œì˜ ì•ë’¤ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
            keywords_list = [k.strip() for k in keywords_str.split(',') if k.strip()]

            # ì›ë³¸ í–‰ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ dict í˜•íƒœë¡œ ë³€í™˜
            original_data = row.to_dict()

            for kw in keywords_list:
                doc_text = f"[{sns_name} - {category}] '{row['title']}' ì˜ìƒì—ì„œ ì–¸ê¸‰ëœ íŠ¸ë Œë“œ í‚¤ì›Œë“œ: {kw}"
                documents.append(doc_text)
                
                metadatas.append({
                    "sns": sns_name,
                    "category": category,
                    "keyword": kw,  # ê°œë³„ í‚¤ì›Œë“œ
                    "updated_at": save_time,
                    **original_data  # ì›ë³¸ í–‰ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
                })
                
                # ID ìƒì„± ì‹œ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì—¬ ê³ ìœ ì„± ë³´ì¥
                unique_id_part = row.get('url', row['title'])
                ids.append(f"{sns_name}_{category}_{unique_id_part}_{kw}")

        if not documents:
            logger.info("â„¹ï¸ ë™ê¸°í™”í•  ìƒˆë¡œìš´ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. DB ë™ê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        self.vector_service.add_documents(documents=documents, metadatas=metadatas, ids=ids)
        logger.info(f"âœ… ë™ê¸°í™” ì™„ë£Œ: {len(documents)}ê°œì˜ ìƒˆë¡œìš´ íŠ¸ë Œë“œ ì§€ì‹ì´ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")